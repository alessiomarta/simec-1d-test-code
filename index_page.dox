/*!

\mainpage Introduction

\section What What is this library?
This library is a basic neural network written in C++ implementing the 1-dimensional SiMEC and SiMExp algorithms from the paper "A singular Riemannian geometry approach to Deep Neural Networks II. Reconstruction of 1-D equivalence classes".

In this version there are no training methods, since there already are other libraries - such as Tensorflow, Caffe, PyTorch or OpenNN - implementing efficient training methods.
At the current development stage including some training methods would only have meant to reinvent the wheel, since the main aim of this version of the library is to test the SiMEC1D and SiMExp1D algorithms. However, this library is not dependent on a particular framework for the training, in the sense that only the weights and the biases from a trained network are needed, along with the structure of the neural network. For more information about the structure of this file, see @subpage wb_file. Given these information, the neural network is then built from scratch, for motivations related to SiMEC_1D and SiMExp_1D algorithms. We also avoid making use of automatic differentiation, which has the unfortunate side effect of slowing down the two algorithms considerably, preferring a hard-coding approach, which allows faster computations.
 
In our examples we employed Keras with Tensorflow as the backend (and we provide the code to extract the weights and the biases from the trained network) but one can feel free to use the framework is most confortable with.

We use the Eigen library to perform the matrix operations. Eigen is provided along with the neural network, as a headers-only library. We store the sparse matrix containing the derivatives of a layer map with respect to the weights and biases in a special reduced form of the Jacobian matrix. See @subpage reduced_form_matrix for the details. We employ the classes VectorXd and MatrixXd, storing the entries as doubles because the precision of the float type is not sufficient to run the SiMEC_1D and SiMExp_1D algorithms with sufficient accuracy.

See https://github.com/alessiomarta/extension_singular_riemannian_framework_code for a python implementation of the n-dimensional SiMEC and SiMExp algorithms with PyTorch.

\section Brief Brief description of the repository
In the repository, which you can find at http://github.com/alessiomarta/simec-1d-test-code there are:
- The library.
- Some python scripts to train the neural network employed for the numerical experiments of the paper "A singular Riemannian geometry approach to Deep Neural Networks II. Reconstruction of 1-D equivalence classes" in the datasets folder. See @subpage num_exps.
- Some python scripts to plot the data generated by the SiMEC and SiMExp algorithms using the aforementioned datasets, in the output folder. See @subpage num_exps. 
- The code (in C++) to run the algorithms. See @subpage test_main.

\section How1 How to compile the test code

To compile the test code we provide a makefile. Open the terminal in the folder in which you find "main.cpp" and type "make". Before to compile modifiy the Makefile to use the instruction set extensions supported by your CPU. The library does not require to install Eigen, which is provided as headers only along with the neural network. However, it requires a previous installation of OpenMP.

Note: In the provided makefile we compile using C++17. You can use previous C++ standards, but consider that Eigen up to version 3.4 is standard C++03 and versions following 3.4 will be C++14.

\section How2 How to use the library in Python

We use pybind11 to generate Python wrappers for the library. The source file containing the wrappers for neural_network.h is py_net.cpp

<b> Eigen3 and pybind11 must be installed on the machine. </b>

To build the Python wrapper (using a Linux-based OS) open a terminal and type:

g++ -O3 -march=native -Ofast -lpthread -DNDEBUG -D_GLIBCXX_PARALLEL -ffast-math -Wall -shared -std=c++17 -Ofast -fno-builtin -ffast-math -mmmx -msse -msse2 -msse3 -msse4 -msse4.1 -msse4.2 -mavx -mavx2 -mfma -frename-registers -m64 -ftree-vectorize -funroll-loops -fopenmp -fopenmp-simd -fPIC $(python3 -m pybind11 --includes) layer.cpp fc_layer_sg.cpp fc_layer_sm.cpp fc_layer_sp.cpp fc_layer_sl.cpp neural_network.cpp activation_functions.cpp read_csv.cpp py_net.cpp -o py_net$(python3-config --extension-suffix)

- We compile using C++17. You can use previous C++ standards, but consider that Eigen up to version 3.4 is standard C++03 and versions following 3.4 will be C++14.
- It may be necessary to add the paths of pybind11 and python if they are not in your compiler's default include paths.
- Before to build the python wrapper, check the instruction set extensions supported by your CPU.

To use the library in Python, import the module py_net.

\section FAQ

 <b>Why don't you employ the Tensor class from Eigen?</b>
 
 In the current implementation we do not use the tensor class form Eigen simply because it is not strictly needed.
 In addition, avoiding tensor at this stage simplifies the testing of SiMEC1D and SiMExp1D algorithms, and allows to follow the notation employed in the paper "A singular Riemannian geometry approach to Deep Neural Networks II. Reconstruction of 1-D equivalence classes.".
  
*/
 
 
//-----------------------------------------------------------



/*! \page num_exps Brief description of the the datasets of the numerical experiments

In this page we briefly describe the datasets and the code for the numerical experiments presented in "A singular Riemannian geometry approach to Deep Neural Networks II. Reconstruction of 1-D equivalence classes". In all the numerical experiments the neural network is learning a function from \f$ \mathbb{R}^2 \f$ to \f$ \mathbb{R} \f$, restricted to a suitable subset: The region in which we generate the features employed for the training. 

The scripts to generate the dataset can be found in the "datasets" folder, containing the following subfolders:

- "surface_1": Contains the Python script to generate the dataset and to train the neural network for the first and fourth numerical experiments ("Learning compact equivalence classes" and "Learning preimages of compact equivalence classes").
- "surface_2" : Contains the Python script to generate the dataset and to train the neural network for the second numerical experiment ("Learning non compact equivalence classes").
- "thermodynamics" : The script "thermodynamics.py" generates the dataset and trains the neural network for the third and fifth numerical experiments ("A thermodynamics problem" and "Thermodynamics: learning a family of isothermal curves"). "true_isothermal.py" plots the true isothermal curve corresponding to the temperature chosen in the paper.
- "classification" : Contains the Python script to generate the dataset and to train the neural network for the sixth numerical experiment ("A classification problem").

Every script saves the structure and the weights/biases of the neural network in "weights.csv" and writes the dataset in another csv file, both in the same folder as the script. In addition some plots of the datasets or of the output of the neural network are created.

To train the neural networks employed in the numerical experiments, we use Keras with TensorFlow as backend.

See also @subpage test_main. 



*/

//-----------------------------------------------------------


/*! \page wb_file On the structure of the file containing the weights and the biases

This library reads the structure, the weights and the biases of a neural network from a text file structured as follows.
The first line of the file contains the number of layers of the network.
Starting from the second line, each layer is described in a block of three consecutive lines in the following way.
- The first line of the block contains (separated by a tab character): the number of rows and columns of the weights matrix of the layer, the dimensions of the input and output spaces and the type of layer. Note that in the current implementation of the neural network, for the supported layer the number of rows coincides with the dimension of the output space and the number of columns coincides with the dimension of the input space. 
- The second line of the block contains the weights of the layer (separated by a tab character) in row-major form.
- The third line of the block contains the biases of the layer (separated by a tab character).


Supported layers:
For the moment the supported feedforward layer are:
- Sigmoid layer "FC_LAYER_SG".
- Softplus layer "FC_LAYER_SP".
- Softmax layer "FC_LAYER_SM".
- SiLu layer "FC_LAYER_SL".

Since in the paper "A singular Riemannian geometry approach to Deep Neural Networks II. Reconstruction of 1-D equivalence classes." the non-smooth ReLu layer has not been discussed yet, we did not implement this kind of layer, at least in the current version of the library.

*/

//-----------------------------------------------------------


/*! \page test_main How to use the example code

To avoid recompiling each time we run a different numerical experiment, the test program reads the information about the dataset and the algorithm to use in the "settings" text file you find in the "init" folder. These data are saved by the program in the settings_info struct. 

\section settings_ The settings file

The "settings" file is a simple text file whose entries are separated by a tab ('\t') character. The test code allows to perform the following tasks:

- To predict the outputs of a dataset.
- To run the SiMEC-1D algorithm, in order to build the equivalence class of a given point.
- To run the SiMExp-1D algorithm, in order to build part of a foliation of the input manifold starting from a given point.

The order of the arguments follows that of the fields in the settings_info struct, namely:

- The name of the file containing the dataset (if none, write '#'). This argument is needed only to run the SiMEC-1D and SiMExp-1D algorithms.
- The size of the input variables.
- The size of the output variables.
- Whether data must be normalized or not ('y'/'n').
- The name of the file containing the structure of the neural network and its weights/biases.
- The name of the file in which the SiMEC algorithms shall write their outputs.
- The number of iterations.
- The delta of the SiMEC-1D algorithm.
- Whether to invert the direction of SiMEC-1D or not ('y'/'n'). Note that there is no privileged direction. The direction in which the curve is built is selected during the first iteration is the direction of the lowest eigenvector of the (degenerate) metric induced on the input manifold , which depends on the particular algorithm employed to find the eigenvectors. This direction correspond to 'y', the opposite one to 'n'. In practice, for 1D equivalence sets, one choice makes the algorithm build a curve from left to right, the other option in the opposite direction. Our advice is to run the algorithm two times using both the options, in order to get a curve in both directions.
- normalize_starting: If SiMEC-1D or SiMExp-1D are selected, this parameter specifies whethere the dataset must be normalized or not (true/false). If true,
  the maximum and the minimum of the dataset the starting point comes from must be provided in a max_mix_struct.
- The file containing the point for which we build the equivalence class.
- The algorithm to run: 'SiMEC-1D' / 'SiMExp-1D' / 'Predict'.
- The epsilon of SiMExp-1D.
- delta_simexp: Delta of the SiMExp-1D algorithm, namely the maximum distance from the starting point.

For example the following settings

\a<#	2  1	n	weights/weights_binary.csv	output/test/simexp_class_out.csv	10000	1.e-3	n	init/starting_point	n	SiMExp-1D	0.1	5.e-6>

make the test program run the SiMExp-1D, for which no dataset is needed, starting from the point in "init/starting_point", initializing the neural network as specified in "weights/weights_binary.csv" and saving the output points "in output/test/simexp_class_out.csv" with the following parameters:

- The size of the input vector is 2.
- The size of the output space is 1.
- Data must not be normalized.
- The algorithm is to be executed for 10000 iterations.
- The Delta of the SiMEC-1D algorithm is 1.e-3
- The direction in which the curve null is constructed is the opposite with respect to the direction of the lowest eigenvector computed in the first iteration. 
- The starting point is not to normalize.
- The epsilon of SiMExp-1D is 0.1.
- The delta of the SiMExp-1D algorithm is 5.e-6.

\section output_ Scripts to plot the results of SiMEC and SiMExp

In the output folder, there are the Python scripts employed to analyze the numerical experiments carried out in the paper "A singular Riemannian geometry approach to Deep Neural Networks II. Reconstruction of 1-D equivalence classes.".

- analysis_exp_1 : Script to generate the plot for the first numerical experiment - "Learning compact equivalence classes".
- analysis_exp_2 : Script to generate the plot for the second numerical experiment - "Learning non compact equivalence classes".
- analysis_thermodynamics: Script to generate the plot for the third numerical experiment - "A thermodynamics problem".
- analysis_exp_annulus : Script to generate the plot for the fourth numerical experiment - "Learning preimages of compact equivalence classes".
- analysis_thermo_between_iso.py : Script to generate the plot for the fifth numerical experiment - "Thermodynamics: learning a family of isothermal curves".
- analysis_binary.py : Script to generate the plot for the sixth numerical experiment - "A classification problem".

*/

//-----------------------------------------------------------


/*! \page reduced_form_matrix On the reduced form of the Jacobian matrices

The Jacobian matrix containing the derivatives with respect the the weights and biases of a fully connected layer is a sparse matrix. For example, let us consider a fully connected layer from \f$ 
\mathbb{R}^{2}\f$ to \f$ \mathbb{R}^{2} \f$ with sigmoid activation function. We can write the map realizing the layer, as:


\f$ \Lambda(\underline{X},\underline{W}_1,b_1,\underline{W}_2,b_2) = 
\begin{pmatrix}
\sigma (\underline{W}_1 \cdot \underline{X} + b_1) \\ 
\sigma (\underline{W}_2 \cdot \underline{X} + b_2)
\end{pmatrix}  
\f$

where \f$\underline{W}_1 = (w_{11},w_{12})\f$ and \f$\underline{W}_2 = (w_{21},w_{22})\f$ are the weights of the layer, \f$b_1,b_2\f$ the biases and \f$\underline{X} = (x_1,x_2)\f$ the input data.
Calling \f$\Lambda_1(\underline{X},\underline{W}_1,b_1)  = \sigma (\underline{W}_1 \cdot \underline{X} + b_1)\f$  and \f$\Lambda_2(\underline{X},\underline{W}_2,b_2)  = \sigma (\underline{W}_2 \cdot \underline{X} + b_2)\f$ the two components of the map \f$ \Lambda\f$, the Jacobian matrix containing the derivatives with respect to the weights and the biases, for a fixed input, is given by


\f[ 
J \Lambda = 
\begin{pmatrix}
\frac{\partial \Lambda_1}{\partial w_{11}} & \frac{\partial \Lambda_1}{\partial w_{12}} & \frac{\partial \Lambda_1}{\partial b_1} & \frac{\partial \Lambda_1}{\partial w_{21}} & \frac{\partial \Lambda_1}{\partial w_{22}} & \frac{\partial \Lambda_1}{\partial b_2}\\ 
\frac{\partial \Lambda_2}{\partial w_{11}} & \frac{\partial \Lambda_2}{\partial w_{12}} & \frac{\partial \Lambda_2}{\partial b_1} & \frac{\partial \Lambda_2}{\partial w_{21}} & \frac{\partial \Lambda_2}{\partial w_{22}} & \frac{\partial \Lambda_2}{\partial b_2}
\end{pmatrix} 
\f]

Considering that \f$\Lambda_1\f$ does not depend on \f$w_{21},w_{22},b_2\f$ and that \f$\Lambda_2\f$ is not a function of \f$w_{11},w_{12},b_1\f$, we find that the Jacobian of \f$ \Lambda
\f$ assumes the form

\f[ 
J \Lambda = 
\begin{pmatrix}
\frac{\partial \Lambda_1}{\partial w_{11}} & \frac{\partial \Lambda_1}{\partial w_{12}} & \frac{\partial \Lambda_1}{\partial b_1} & 0 & 0 & 0\\ 
0 & 0 & 0 & \frac{\partial \Lambda_2}{\partial w_{21}} & \frac{\partial \Lambda_2}{\partial w_{22}} & \frac{\partial \Lambda_2}{\partial b_2}
\end{pmatrix} 
\f]
 
Now let us consider a generic fully connected layers with n nodes, whose input is a vector space of dimension m. Then, following the , the map realizing the layer seen as a function of the weights and biases, is a vector valued function from \f$ \mathbb{R}^{n \cdot (m+1)} \f$ to \f$ \mathbb{R}^{n} \f$.The Jacobian matrix of this layer is a \f$ m \times (n \cdot m)\f$ matrix with the following structure:
- In the first row, only the first \f$ m+1 \f$ entries are in general non-null.
- In the second row, the first \f$ m+1 \f$ entries are null, then the next block of \f$ m+1 \f$ elements are non-null. The remainder of this row contains only zeroes.
- In the k-th row, the first \f$ (k-1)*(m+1) \f$ entries are null, then we find \f$ m+1 \f$ non-null entries and finally the rest of the rows is made of zeroes.
Therefore, to save space (and computation time) we store only the non-null entries in a \f$ m \times n\f$ matrix that we called the reduced form of the Jacobian, or reduced matrix for short. 

In order to compute the pullback of the metric, however, we need to compute some products between the Jacobian matrices. To this end, we implement the functions reduced_standard_mul and standard_reduced_mul contained in matrix_utils.h, computing the product between a reduced matrix and a standard one (reduced_standard_mul) and the product bewteen a standard matrix and a reduced one
(standard_reduced_mul). The conversion between the standard and the reduced forms of the Jacobian matrix are handled by the reduced_to_standard and standard_to_reduced functions.
*/




