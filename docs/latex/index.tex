\hypertarget{index_What}{}\doxysection{What is this library?}\label{index_What}
This library is a basic neural network written in C++ implementing the 1-\/dimensional Si\+M\+EC and Si\+M\+Exp algorithms from the paper \char`\"{}\+A singular Riemannian geometry approach to Deep Neural Networks I\+I. Reconstruction of 1-\/\+D equivalence classes\char`\"{}.

In this version there are no training methods, since there already are other libraries -\/ such as Tensorflow, Caffe, Py\+Torch or Open\+NN -\/ implementing efficient training methods. At the current development stage including some training methods would only have meant to reinvent the wheel, since the main aim of this version of the library is to test the Si\+M\+E\+C1D and Si\+M\+Exp1D algorithms. However, this library is not dependent on a particular framework for the training, in the sense that only the weights and the biases from a trained network are needed, along with the structure of the neural network. For more information about the structure of this file, see \mbox{\hyperlink{wb_file}{On the structure of the file containing the weights and the biases}}. Given these information, the neural network is then built from scratch, for motivations related to Si\+M\+E\+C\+\_\+1D and Si\+M\+Exp\+\_\+1D algorithms. We also avoid making use of automatic differentiation, which has the unfortunate side effect of slowing down the two algorithms considerably, preferring a hard-\/coding approach, which allows faster computations.

In our examples we employed Keras with Tensorflow as the backend (and we provide the code to extract the weights and the biases from the trained network) but one can feel free to use the framework is most confortable with.

We use the Eigen library to perform the matrix operations. Eigen is provided along with the neural network, as a headers-\/only library. We store the sparse matrix containing the derivatives of a layer map with respect to the weights and biases in a special reduced form of the Jacobian matrix. See \mbox{\hyperlink{reduced_form_matrix}{On the reduced form of the Jacobian matrices}} for the details. We employ the classes Vector\+Xd and Matrix\+Xd, storing the entries as doubles because the precision of the float type is not sufficient to run the Si\+M\+E\+C\+\_\+1D and Si\+M\+Exp\+\_\+1D algorithms with sufficient accuracy.\hypertarget{index_Brief}{}\doxysection{Brief description of the repository}\label{index_Brief}
In the repository, which you can find at \href{http://github.com/alessiomarta/simec-1d-test-code}{\texttt{ http\+://github.\+com/alessiomarta/simec-\/1d-\/test-\/code}} there are\+:
\begin{DoxyItemize}
\item The library.
\item Some python scripts to train the neural network employed for the numerical experiments of the paper \char`\"{}\+A singular Riemannian geometry approach to Deep Neural Networks I\+I. Reconstruction of 1-\/\+D equivalence classes\char`\"{} in the datasets folder. See \mbox{\hyperlink{num_exps}{Brief description of the the datasets of the numerical experiments}}.
\item Some python scripts to plot the data generated by the Si\+M\+EC and Si\+M\+Exp algorithms using the aforementioned datasets, in the output folder. See \mbox{\hyperlink{num_exps}{Brief description of the the datasets of the numerical experiments}}.
\item The code (in C++) to run the algorithms. See \mbox{\hyperlink{test_main}{How to use the example code}}.
\end{DoxyItemize}\hypertarget{index_How1}{}\doxysection{How to compile the test code}\label{index_How1}
To compile the test code we provide a makefile. Open the terminal in the folder in which you find \char`\"{}main.\+cpp\char`\"{} and type \char`\"{}make\char`\"{}. Before to compile modifiy the Makefile to use the instruction set extensions supported by your C\+PU. The library does not require to install Eigen, which is provided as headers only along with the neural network. However, it requires a previous installation of Open\+MP.

Note\+: In the provided makefile we compile using C++17. You can use previous C++ standards, but consider that Eigen up to version 3.\+4 is standard C++03 and versions following 3.\+4 will be C++14.\hypertarget{index_How2}{}\doxysection{How to use the library in Python}\label{index_How2}
We use pybind11 to generate Python wrappers for the library. The source file containing the wrappers for \mbox{\hyperlink{neural__network_8h_source}{neural\+\_\+network.\+h}} is py\+\_\+net.\+cpp

{\bfseries{ Eigen3 and pybind11 must be installed on the machine. }}

To build the Python wrapper (using a Linux-\/based OS) open a terminal and type\+:

g++ -\/O3 -\/march=native -\/Ofast -\/lpthread -\/D\+N\+D\+E\+B\+UG -\/D\+\_\+\+G\+L\+I\+B\+C\+X\+X\+\_\+\+P\+A\+R\+A\+L\+L\+EL -\/ffast-\/math -\/Wall -\/shared -\/std=c++17 -\/Ofast -\/fno-\/builtin -\/ffast-\/math -\/mmmx -\/msse -\/msse2 -\/msse3 -\/msse4 -\/msse4.\+1 -\/msse4.\+2 -\/mavx -\/mavx2 -\/mfma -\/frename-\/registers -\/m64 -\/ftree-\/vectorize -\/funroll-\/loops -\/fopenmp -\/fopenmp-\/simd -\/f\+P\+IC \$(python3 -\/m pybind11 --includes) layer.\+cpp fc\+\_\+layer\+\_\+sg.\+cpp fc\+\_\+layer\+\_\+sm.\+cpp fc\+\_\+layer\+\_\+sp.\+cpp fc\+\_\+layer\+\_\+sl.\+cpp neural\+\_\+network.\+cpp activation\+\_\+functions.\+cpp read\+\_\+csv.\+cpp py\+\_\+net.\+cpp -\/o py\+\_\+net\$(python3-\/config --extension-\/suffix)


\begin{DoxyItemize}
\item We compile using C++17. You can use previous C++ standards, but consider that Eigen up to version 3.\+4 is standard C++03 and versions following 3.\+4 will be C++14.
\item It may be necessary to add the paths of pybind11 and python if they are not in your compiler\textquotesingle{}s default include paths.
\item Before to build the python wrapper, check the instruction set extensions supported by your C\+PU.
\end{DoxyItemize}

To use the library in Python, import the module py\+\_\+net.\hypertarget{index_FAQ}{}\doxysection{F\+AQ}\label{index_FAQ}
{\bfseries{Why don\textquotesingle{}t you employ the Tensor class from Eigen?}}

In the current implementation we do not use the tensor class form Eigen simply because it is not strictly needed. In addition, avoiding tensor at this stage simplifies the testing of Si\+M\+E\+C1D and Si\+M\+Exp1D algorithms, and allows to follow the notation employed in the paper \char`\"{}\+A singular Riemannian geometry approach to Deep Neural Networks I\+I. Reconstruction of 1-\/\+D equivalence classes.\char`\"{}. 